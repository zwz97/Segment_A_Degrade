{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# SAM2 Evaluation Pipeline - Colab Notebook\n",
    "\n",
    "This notebook provides an interactive way to run the SAM2 evaluation pipeline\n",
    "\n",
    "**Workflow:**\n",
    "1.  **Setup:** Clone repository, install dependencies, and install the required SAM2 library.\n",
    "2.  **Configuration:** Set parameters for the pipeline (model, data paths, etc.).\n",
    "3.  **Data Preparation:** Generate the `degradation_map.json` (assumes image data exists).\n",
    "4.  **(Optional) Visualization:** Inspect sample images and masks.\n",
    "5.  **Run Pipeline:** Execute the evaluation using the configured settings.\n",
    "6.  **View Results:** Load and display the output CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL TO INSERT (after cell 1, before cell 2)\n",
    "\n",
    "# --- OPTIONAL: Mount Google Drive ---\n",
    "# Uncomment and run this cell if your data directory (or parts of it)\n",
    "# resides on Google Drive instead of being in the Git repo.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# --- Optional: Create symlink if needed ---\n",
    "# If your data is on Drive, e.g., at /content/drive/MyDrive/SAM2_data\n",
    "# and your code expects it at ./data, you might create a symlink:\n",
    "# DRIVE_DATA_PATH = '/content/drive/MyDrive/SAM2_data' # <-- Adjust this path\n",
    "# PROJECT_DATA_PATH = os.path.join(PROJECT_ROOT, 'data')\n",
    "# if IN_COLAB and not os.path.exists(PROJECT_DATA_PATH):\n",
    "#     print(f\"Linking {DRIVE_DATA_PATH} to {PROJECT_DATA_PATH}...\")\n",
    "#     !ln -s \"$DRIVE_DATA_PATH\" \"$PROJECT_DATA_PATH\"\n",
    "# else:\n",
    "#    print(\"Skipping symlink creation (not in Colab or data path exists).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Base directory for the project\n",
    "# If in Colab, clone the repo. Otherwise, assume we are running from the repo root.\n",
    "import os\n",
    "\n",
    "if IN_COLAB:\n",
    "    print('Running in Colab, cloning repository...')\n",
    "    # !!! IMPORTANT: Replace the URL below with your actual repository URL !!!\n",
    "    # If your repository is private, use a PAT (Personal Access Token) in the URL:\n",
    "    # !git clone https://<YOUR_GITHUB_TOKEN>@github.com/<YOUR-USERNAME>/SAM2_analysis.git\n",
    "    !git clone https://github.com/<YOUR-ORG-OR-USERNAME>/SAM2_analysis.git # <-- EDIT THIS LINE\n",
    "    %cd SAM2_analysis\n",
    "    PROJECT_ROOT = '/content/SAM2_analysis'\n",
    "else:\n",
    "    print('Running locally, assuming current directory is project root.')\n",
    "    # Find the project root assuming this notebook is in the root\n",
    "    PROJECT_ROOT = os.path.abspath('.')\n",
    "    # Verify by checking for a known file/directory\n",
    "    if not os.path.exists(os.path.join(PROJECT_ROOT, 'main.py')):\n",
    "        print(f'Warning: Could not confirm project root at {PROJECT_ROOT}')\n",
    "\n",
    "print(f'Project Root: {PROJECT_ROOT}')\n",
    "os.chdir(PROJECT_ROOT) # Ensure we are in the project root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies from requirements.txt\n",
    "print('\\nInstalling dependencies...')\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the SAM2 library\n",
    "# Assumes the sam2 code is located in 'external/sam2' within the project\n",
    "print('\\nInstalling SAM2 library...')\n",
    "SAM2_DIR = os.path.join(PROJECT_ROOT, 'external/sam2')\n",
    "\n",
    "if not os.path.exists(SAM2_DIR):\n",
    "    print(f'Error: SAM2 directory not found at {SAM2_DIR}')\n",
    "    print('Please ensure you have cloned the SAM2 repository into external/sam2')\n",
    "    # Optional: Add command to clone it if missing\n",
    "    # print('Attempting to clone SAM2...')\n",
    "    # !git clone <SAM2_REPO_URL> external/sam2 # <-- Add SAM2 repo URL if desired\n",
    "else:\n",
    "    # Editable install. The bash $SAM2_DIR expands correctly in a shell context. - makes code directly importable\n",
    "    %pip install -e {SAM2_DIR}\n",
    "\n",
    "# --- OPTIONAL (only if SAM2_DIR is missing): ---------\n",
    "if not os.path.exists(SAM2_DIR):\n",
    "    print('Cloning official SAM2 repo â€¦')\n",
    "    !git clone https://github.com/facebookresearch/sam2.git \"$SAM2_DIR\"\n",
    "    %pip install -e \"$SAM2_DIR\"\n",
    "# -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pipeline Configuration ---\n",
    "# Mimic the structure of sam2_eval_config.json\n",
    "\n",
    "config = {\n",
    "    \"pipeline_name\": \"sam2_eval\",\n",
    "    \"description\": \"Evaluate SAM2 auto-mask generator on data map (Colab)\",\n",
    "\n",
    "    # --- Data Configuration ---\n",
    "    # Path to the generated data map (relative to project root)\n",
    "    \"data_path\": \"data/degradation_map.json\",\n",
    "\n",
    "    # Base directory where image files referenced in data_path are located\n",
    "    # IMPORTANT: Filepaths within degradation_map.json (e.g., \"images/gt_img/1.jpg\")\n",
    "    # are treated as relative to this 'image_base_dir'.\n",
    "    \"image_base_dir\": \"data\", # Results in absolute path like /content/SAM2_analysis/data\n",
    "\n",
    "    # --- Model Configuration ---\n",
    "    # Hugging Face identifier for the SAM2 model\n",
    "    # Examples: 'facebook/sam2-hiera-tiny', 'facebook/sam2-hiera-small',\n",
    "    #           'facebook/sam2-hiera-base', 'facebook/sam2-hiera-large'\n",
    "    \"model_hf_id\": \"facebook/sam2-hiera-tiny\", # Use a smaller model for faster testing\n",
    "\n",
    "    # --- Mask Generator Configuration ---\n",
    "    # Parameters passed to SAM2AutomaticMaskGenerator\n",
    "    # See SAM2 library documentation for all options\n",
    "    \"generator_config\": {\n",
    "        \"points_per_side\": 16,       # Lower for faster processing\n",
    "        \"pred_iou_thresh\": 0.80,     # Default: 0.88\n",
    "        \"stability_score_thresh\": 0.90, # Default: 0.95\n",
    "        \"crop_n_layers\": 0,          # Default: 0 (no cropping)\n",
    "        \"min_mask_region_area\": 10   # Default: 0\n",
    "    },\n",
    "\n",
    "    # --- Evaluation Metric Configuration ---\n",
    "   #\"iou_threshold\": 0.5,        # No longer directly used by pipeline func, keep/remove as needed for analysis\n",
    "    \"bf1_tolerance\": 2,          # Tolerance in pixels for Boundary F1 score\n",
    "\n",
    "    # --- Output Configuration ---\n",
    "    # Path template for the results CSV file (relative to project root)\n",
    "    # The pipeline function will add a timestamp.\n",
    "    \"output_path\": \"output/results_colab.csv\" # Combine dir and prefix into a template\n",
    "}\n",
    "\n",
    "# Make directories/paths absolute for clarity later\n",
    "config['data_path'] = os.path.join(PROJECT_ROOT, config['data_path'])\n",
    "config['image_base_dir'] = os.path.join(PROJECT_ROOT, config['image_base_dir'])\n",
    "# --- Make the output_path absolute ---\n",
    "config['output_path'] = os.path.join(PROJECT_ROOT, config['output_path'])\n",
    "# --- Remove the old output_dir line ---\n",
    "# config['output_dir'] = os.path.join(PROJECT_ROOT, config['output_dir']) # REMOVED\n",
    "\n",
    "print(\"Configuration set:\")\n",
    "import json\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "**IMPORTANT:** This section assumes the necessary image and annotation files are already present in the `data/images/gt_img/` directory and any corresponding degraded images are in `data/images/img_degraded/` within your Colab environment or mounted drive.\n",
    "\n",
    "The `build_local_map.py` script will scan these directories to create the `degradation_map.json`.\n",
    "\n",
    "If the source images/annotations are not present, you need to:\n",
    "1. Generate or place the original images and annotations in `data/images/gt_img/`.\n",
    "2. (Optional) Generate degraded images using `code_degradation.py` or place them manually into the correct subdirectories within `data/images/img_degraded/`.\n",
    "3. Upload/sync the populated `data/images/` structure to Colab or Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Ensure os is imported if running cells independently\n",
    "\n",
    "# Ensure data directories exist (though the script expects content within them)\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, 'data')\n",
    "IMAGES_GT_DIR = os.path.join(DATA_DIR, 'images', 'gt_img')\n",
    "IMAGES_DEGRADED_DIR = os.path.join(DATA_DIR, 'images', 'img_degraded')\n",
    "OUTPUT_DIR = os.path.dirname(config['output_path']) # absolute path\n",
    "\n",
    "os.makedirs(IMAGES_GT_DIR, exist_ok=True) # Create base dirs if they don't exist\n",
    "os.makedirs(IMAGES_DEGRADED_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f'Checking for required image directories:')\n",
    "print(f'- Ground Truth Images: {IMAGES_GT_DIR}')\n",
    "print(f'- Degraded Images: {IMAGES_DEGRADED_DIR}')\n",
    "print(f'Expected output map path: {config[\"data_path\"]}')\n",
    "\n",
    "# Check if the essential input directories for build_local_map.py exist\n",
    "if not os.path.exists(IMAGES_GT_DIR) or not os.listdir(IMAGES_GT_DIR):\n",
    "     print(f\"Warning: Ground truth image directory '{IMAGES_GT_DIR}' does not exist or is empty. \"\n",
    "           f\"The 'build_local_map.py' script will likely fail or produce an empty map.\")\n",
    "# Note: img_degraded might be optional depending on use case, so we don't warn if it's missing/empty\n",
    "\n",
    "# Run the script to generate the degradation_map.json\n",
    "print('\\nRunning script to generate degradation_map.json...')\n",
    "# Assumes build_local_map.py reads from ../images/gt_img and ../images/img_degraded relative to its own location\n",
    "!python data/data_scripts/build_local_map.py\n",
    "\n",
    "# Verify the map was created\n",
    "data_map_path = config['data_path'] # Use absolute path from config\n",
    "if os.path.exists(data_map_path):\n",
    "    # Optionally check if the map is non-empty\n",
    "    try:\n",
    "        with open(data_map_path, 'r') as f:\n",
    "            map_content = json.load(f)\n",
    "        if map_content:\n",
    "             print(f'Successfully generated non-empty {data_map_path}')\n",
    "        else:\n",
    "             print(f'Successfully generated {data_map_path}, but it appears to be empty.')\n",
    "    except Exception as e:\n",
    "         print(f'Successfully generated {data_map_path}, but could not verify content: {e}')\n",
    "else:\n",
    "    print(f'Error: {data_map_path} was not generated. Check data availability and script output.')\n",
    "    # Add more detailed error checking if the script provides specific logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 4. (Optional) Visualize Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pycocotools.mask as mask_util # Import for RLE decoding\n",
    "import os # Ensure os is imported\n",
    "\n",
    "def visualize_sample(data_map_path, image_base_dir):\n",
    "    \"\"\"Loads the data map, picks a random image, and displays its versions and GT mask.\"\"\"\n",
    "    if not os.path.exists(data_map_path):\n",
    "        print(f'Cannot visualize: {data_map_path} not found.')\n",
    "        return\n",
    "\n",
    "    with open(data_map_path, 'r') as f:\n",
    "        try:\n",
    "            data_map = json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error reading data map JSON: {e}\")\n",
    "            return\n",
    "\n",
    "\n",
    "    if not data_map:\n",
    "        print('Cannot visualize: Data map is empty.')\n",
    "        return\n",
    "\n",
    "    image_id = random.choice(list(data_map.keys()))\n",
    "    print(f'Visualizing sample for image_id: {image_id}')\n",
    "    item_data = data_map[image_id]\n",
    "\n",
    "    # Decode GT mask\n",
    "    gt_rle = item_data.get('ground_truth_rle')\n",
    "    gt_mask = None\n",
    "    if gt_rle:\n",
    "        try:\n",
    "            # Handle potential string vs dict RLE formats if needed\n",
    "            if isinstance(gt_rle, str): # If RLE is just the counts string\n",
    "                 # Need size info - assume it's stored elsewhere or reconstruct\n",
    "                 print(\"Warning: GT RLE is string, size info needed for decoding.\")\n",
    "                 # Example: Need to fetch item_data['height'], item_data['width']\n",
    "                 # gt_rle_dict = {'size': [item_data['height'], item_data['width']], 'counts': gt_rle}\n",
    "                 # gt_mask = mask_util.decode(gt_rle_dict)\n",
    "            elif isinstance(gt_rle, dict):\n",
    "                 gt_mask = mask_util.decode(gt_rle)\n",
    "            else:\n",
    "                 print(f\"Warning: Unexpected GT RLE format: {type(gt_rle)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'  Could not decode GT RLE: {e}')\n",
    "\n",
    "\n",
    "    # Count versions - needs robust handling of structure\n",
    "    num_versions = 0\n",
    "    versions_to_plot = []\n",
    "    base_img_path = image_base_dir # Already absolute\n",
    "\n",
    "    if 'versions' in item_data:\n",
    "         for degradation_type, levels_or_data in item_data['versions'].items():\n",
    "              if isinstance(levels_or_data, dict) and 'filepath' in levels_or_data: # e.g., 'original'\n",
    "                   filepath = levels_or_data['filepath']\n",
    "                   level = levels_or_data.get('level', 'N/A')\n",
    "                   title = f'{degradation_type}\\n(Level: {level})'\n",
    "                   # Construct absolute path carefully based on structure\n",
    "                   abs_path = os.path.join(base_img_path, filepath) # Assumes filepath is relative to base_img_dir\n",
    "                   versions_to_plot.append({'title': title, 'path': abs_path})\n",
    "                   num_versions += 1\n",
    "              elif isinstance(levels_or_data, dict): # Nested levels like {'1': {...}, '2': {...}}\n",
    "                   for level, version_data in levels_or_data.items():\n",
    "                       if isinstance(version_data, dict) and 'filepath' in version_data:\n",
    "                           filepath = version_data['filepath']\n",
    "                           level_val = version_data.get('level', level) # Use nested level if available\n",
    "                           title = f'{degradation_type}_{level}\\n(Level: {level_val})'\n",
    "                           # Construct absolute path\n",
    "                           abs_path = os.path.join(base_img_path, filepath) # Assumes filepath relative to base\n",
    "                           versions_to_plot.append({'title': title, 'path': abs_path})\n",
    "                           num_versions += 1\n",
    "\n",
    "    plot_cols = num_versions + (1 if gt_mask is not None else 0)\n",
    "    if plot_cols == 0:\n",
    "        print(\"No image versions or GT mask found to plot.\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(1, max(1, plot_cols), figsize=(5 * max(1, plot_cols), 5))\n",
    "    if plot_cols == 1:\n",
    "        axes = [axes] # Make it iterable\n",
    "\n",
    "    plot_idx = 0\n",
    "\n",
    "    # Display versions\n",
    "    for version_info in versions_to_plot:\n",
    "        img_path = version_info['path']\n",
    "        title = version_info['title']\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            axes[plot_idx].imshow(img)\n",
    "            axes[plot_idx].set_title(title)\n",
    "        except FileNotFoundError:\n",
    "            print(f'  Image not found: {img_path}')\n",
    "            axes[plot_idx].set_title(f'{title}\\n(Not Found)')\n",
    "        except Exception as e:\n",
    "             print(f\"Error loading image {img_path}: {e}\")\n",
    "             axes[plot_idx].set_title(f'{title}\\n(Load Error)')\n",
    "        finally:\n",
    "            axes[plot_idx].axis('off')\n",
    "            plot_idx += 1\n",
    "\n",
    "\n",
    "    # Display GT mask\n",
    "    if gt_mask is not None:\n",
    "        if plot_idx < len(axes): # Ensure we don't go out of bounds\n",
    "            axes[plot_idx].imshow(gt_mask, cmap='gray')\n",
    "            axes[plot_idx].set_title('Ground Truth Mask')\n",
    "            axes[plot_idx].axis('off')\n",
    "        else:\n",
    "             print(\"Warning: Not enough subplot axes allocated for GT mask.\")\n",
    "\n",
    "    # Hide unused axes\n",
    "    for i in range(plot_idx + (1 if gt_mask is not None else 0), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Run visualization ---\n",
    "data_map_path = config['data_path']\n",
    "image_base_dir = config['image_base_dir']\n",
    "if os.path.exists(data_map_path):\n",
    "    visualize_sample(data_map_path, image_base_dir)\n",
    "else:\n",
    "    print(f\"Skipping visualization because data map not found: {data_map_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL TO INSERT (before cell under \"## 5. Run Pipeline\")\n",
    "\n",
    "# --- OPTIONAL: Hugging Face Login ---\n",
    "# Uncomment and run this cell if the model you specified in the config\n",
    "# (\"model_hf_id\") is private and requires authentication.\n",
    "# Replace YOUR_HF_TOKEN with your actual Hugging Face access token.\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"YOUR_HF_TOKEN\") # Or use !huggingface-cli login --token YOUR_HF_TOKEN\n",
    "\n",
    "# Or, if using notebook_login:\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login() # Prompts for token interactively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 5. Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os # Ensure os is imported\n",
    "\n",
    "# Ensure project root is in path for imports\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# Import the main pipeline function\n",
    "try:\n",
    "    from sam2_eval_pipeline import run_evaluation_pipeline\n",
    "    print('Imported run_evaluation_pipeline successfully.')\n",
    "except ImportError as e:\n",
    "    print(f'Error importing pipeline function: {e}')\n",
    "    print('Ensure installation steps completed correctly and you are in the project root.')\n",
    "    run_evaluation_pipeline = None # Prevent further errors\n",
    "except Exception as e:\n",
    "     print(f\"An unexpected error occurred during import: {e}\")\n",
    "     run_evaluation_pipeline = None\n",
    "\n",
    "\n",
    "# Execute the pipeline\n",
    "results_df = None # Initialize as None, as the function doesn't return the df\n",
    "if run_evaluation_pipeline:\n",
    "    print('\\nStarting evaluation pipeline...') \n",
    "    try:\n",
    "        # --- Corrected Call: Pass the entire config dictionary ---\n",
    "        run_evaluation_pipeline(config)\n",
    "\n",
    "        # --- Update Log Message ---\n",
    "        # Get the directory from the output_path template for the log message\n",
    "        output_dir_for_log = os.path.dirname(config['output_path'])\n",
    "        print(f'Pipeline finished. Results should be saved in {output_dir_for_log}')\n",
    "\n",
    "        # Since the function doesn't return the DF, keep results_df as None\n",
    "        # The next cell will load the CSV from the file.\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during pipeline execution: {e}\")\n",
    "\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print detailed traceback for debugging\n",
    "else:\n",
    "    print(\"Skipping pipeline execution due to import failure.\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 6. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import datetime # Needed if you want to parse timestamps, though not strictly required for just finding the latest\n",
    "\n",
    "# --- Get the expected output pattern from the config ---\n",
    "# config['output_path'] is like: /content/SAM2_analysis/output/results_colab.csv\n",
    "output_path_template = config['output_path']\n",
    "output_dir = os.path.dirname(output_path_template)\n",
    "base_filename = os.path.basename(output_path_template)\n",
    "filename_stem, filename_ext = os.path.splitext(base_filename) # e.g., \"results_colab\", \".csv\"\n",
    "\n",
    "# The pipeline likely creates filenames like: results_colab_YYYYMMDD_HHMMSS.csv\n",
    "# We need to find the latest file matching this pattern\n",
    "search_pattern = os.path.join(output_dir, f\"{filename_stem}_*{filename_ext}\") # e.g., /content/.../output/results_colab_*.csv\n",
    "\n",
    "print(f\"Searching for results files matching: {search_pattern}\")\n",
    "\n",
    "try:\n",
    "    # Find all matching files\n",
    "    result_files = glob.glob(search_pattern)\n",
    "\n",
    "    if not result_files:\n",
    "        print(\"No results files found matching the pattern.\")\n",
    "        print(\"Ensure the pipeline ran successfully and created an output file.\")\n",
    "    else:\n",
    "        # Find the most recently modified file\n",
    "        latest_file = max(result_files, key=os.path.getmtime)\n",
    "        print(f\"Loading latest results file: {latest_file}\")\n",
    "\n",
    "        # Load the CSV\n",
    "        results_df_loaded = pd.read_csv(latest_file)\n",
    "        print(\"\\\\nDisplaying loaded results DataFrame:\")\n",
    "\n",
    "        # Display using Colab's interactive table if available\n",
    "        try:\n",
    "            from google.colab.data_table import DataTable\n",
    "            display(DataTable(results_df_loaded))\n",
    "        except ImportError:\n",
    "            display(results_df_loaded) # Fallback for non-Colab\n",
    "\n",
    "except FileNotFoundError:\n",
    "     print(f\"Error: Output directory '{output_dir}' not found.\")\n",
    "except Exception as e:\n",
    "     print(f\"An error occurred while loading or displaying results: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
